A cool little Google research paper. Mostly showing off a dataset but doing some research review work as well. Not too much to say about it, but I will list some of the methods they used below just in case you would like to read a little summary on them and see how they preformed:

* LR with BOW
* Averaged Embeddings
* Paragraph Vector
* LSTM Reader
* Attentive Reader
* Memory Network
* Seq2seq
* Seq2seq with placeholders
* Character seq2seq
* Pretrained character seq2seq

Somewhat funny, but I have actually read a lot of the above O_O
