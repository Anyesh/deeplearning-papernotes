Progressive nets, pretty simple one from deep mind. The idea here is to have a NN with multiple layers trained. Instead of fine-tuning for a new task, we make a whole new network that takes as an input its previous layers as well as the first network’s previous layer. The first network’s parameters will be frozen for the training of the second one. Yep that’s it. Works pretty well for training from simulation and then generalizing to real. 
