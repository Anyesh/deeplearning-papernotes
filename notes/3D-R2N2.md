A Stanford work on voxel reconstruction using multiple views of images. 

The network is a bit complex so bear with me. It begins be embedding the images using a 2D CNN (basically a resnet). This embedding (1028 dims) is then passed into a 3D convolutional LSTM (a mouthful). The LSTM units are arranged in 3D such that each LSTM is responsible for for reconstructing a single part of the the final output. These LSTMs work like normal convolutional LSTMs except they have access to their neighbors’ hidden states in 3D. We then take the hidden states from all the LSTMs and we feed that to a deconvolutional NN to give us the goodies in the end. 

The network is interesting and I am happy it uses multiple views, but the results are beaten by H. Su’s work.
